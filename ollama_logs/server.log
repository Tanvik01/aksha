2025/04/04 04:47:24 routes.go:1231: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\Madhav\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-04-04T04:47:24.461+05:30 level=INFO source=images.go:458 msg="total blobs: 0"
time=2025-04-04T04:47:24.461+05:30 level=INFO source=images.go:465 msg="total unused blobs removed: 0"
time=2025-04-04T04:47:24.462+05:30 level=INFO source=routes.go:1298 msg="Listening on 127.0.0.1:11434 (version 0.6.4)"
time=2025-04-04T04:47:24.462+05:30 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-04-04T04:47:24.462+05:30 level=INFO source=gpu_windows.go:167 msg=packages count=1
time=2025-04-04T04:47:24.462+05:30 level=INFO source=gpu_windows.go:214 msg="" package=0 cores=6 efficiency=0 threads=12
time=2025-04-04T04:47:24.657+05:30 level=INFO source=gpu.go:319 msg="detected OS VRAM overhead" id=GPU-4ff58a81-d5fa-5a5b-d5a9-904c06357fa6 library=cuda compute=8.6 driver=12.5 name="NVIDIA GeForce RTX 3050 Laptop GPU" overhead="532.1 MiB"
time=2025-04-04T04:47:24.663+05:30 level=INFO source=types.go:130 msg="inference compute" id=GPU-4ff58a81-d5fa-5a5b-d5a9-904c06357fa6 library=cuda variant=v12 compute=8.6 driver=12.5 name="NVIDIA GeForce RTX 3050 Laptop GPU" total="4.0 GiB" available="3.2 GiB"
[GIN] 2025/04/04 - 04:47:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 04:47:30 | 404 |       999.2µs |       127.0.0.1 | POST     "/api/show"
time=2025-04-04T04:47:31.470+05:30 level=INFO source=download.go:177 msg="downloading e8ad13eff07a in 16 509 MB part(s)"
time=2025-04-04T04:48:03.708+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 2 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T04:48:04.706+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 4 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T04:48:20.707+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 6 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T04:48:26.707+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 0 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T04:48:52.707+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 13 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T04:48:54.706+05:30 level=INFO source=download.go:374 msg="e8ad13eff07a part 5 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
[GIN] 2025/04/04 - 04:49:56 | 200 |         2m26s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/04 - 04:50:13 | 200 |       500.2µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 04:50:13 | 404 |       500.1µs |       127.0.0.1 | POST     "/api/show"
time=2025-04-04T04:50:14.854+05:30 level=INFO source=download.go:177 msg="downloading aeda25e63ebd in 16 208 MB part(s)"
time=2025-04-04T04:52:44.119+05:30 level=INFO source=download.go:374 msg="aeda25e63ebd part 3 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-04-04T05:03:14.231+05:30 level=INFO source=download.go:177 msg="downloading e0a42594d802 in 1 358 B part(s)"
time=2025-04-04T05:03:15.751+05:30 level=INFO source=download.go:177 msg="downloading dd084c7d92a3 in 1 8.4 KB part(s)"
time=2025-04-04T05:03:17.247+05:30 level=INFO source=download.go:177 msg="downloading 3116c5225075 in 1 77 B part(s)"
time=2025-04-04T05:03:18.749+05:30 level=INFO source=download.go:177 msg="downloading b6ae5839783f in 1 489 B part(s)"
[GIN] 2025/04/04 - 05:03:26 | 200 |        13m12s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/04 - 05:03:26 | 200 |     92.9469ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-04T05:03:26.292+05:30 level=INFO source=server.go:105 msg="system memory" total="13.9 GiB" free="6.2 GiB" free_swap="16.9 GiB"
time=2025-04-04T05:03:26.294+05:30 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=0 layers.split="" memory.available="[3.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.6 GiB" memory.required.partial="0 B" memory.required.kv="214.0 MiB" memory.required.allocations="[0 B]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-04-04T05:03:26.396+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T05:03:26.408+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T05:03:26.408+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T05:03:26.408+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T05:03:26.408+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T05:03:26.408+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T05:03:26.411+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="C:\\Users\\Madhav\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Madhav\\.ollama\\models\\blobs\\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 --threads 6 --no-mmap --parallel 1 --port 51787"
time=2025-04-04T05:03:26.480+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-04T05:03:26.480+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-04T05:03:26.480+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-04T05:03:26.509+05:30 level=INFO source=runner.go:821 msg="starting ollama engine"
time=2025-04-04T05:03:26.511+05:30 level=INFO source=runner.go:884 msg="Server listening on 127.0.0.1:51787"
time=2025-04-04T05:03:26.608+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
time=2025-04-04T05:03:26.608+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
time=2025-04-04T05:03:26.608+05:30 level=INFO source=ggml.go:66 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
time=2025-04-04T05:03:26.731+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from C:\Users\Madhav\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
time=2025-04-04T05:03:26.813+05:30 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-04-04T05:03:26.817+05:30 level=INFO source=ggml.go:288 msg="model weights" buffer=CPU size="3.6 GiB"
time=2025-04-04T05:03:28.671+05:30 level=INFO source=ggml.go:380 msg="compute graph" backend=CPU buffer_type=CPU
time=2025-04-04T05:03:28.677+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T05:03:28.688+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T05:03:28.688+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T05:03:28.688+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T05:03:28.688+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T05:03:28.688+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T05:03:28.738+05:30 level=INFO source=server.go:619 msg="llama runner started in 2.26 seconds"
[GIN] 2025/04/04 - 05:03:28 | 200 |    2.5988598s |       127.0.0.1 | POST     "/api/generate"
time=2025-04-04T05:08:33.754+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0136168 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T05:08:34.004+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2638384 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T05:08:34.253+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5133798 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T05:13:18.071+05:30 level=INFO source=server.go:105 msg="system memory" total="13.9 GiB" free="6.6 GiB" free_swap="16.9 GiB"
time=2025-04-04T05:13:18.072+05:30 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=0 layers.split="" memory.available="[3.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.6 GiB" memory.required.partial="0 B" memory.required.kv="214.0 MiB" memory.required.allocations="[0 B]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-04-04T05:13:18.176+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T05:13:18.187+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T05:13:18.187+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T05:13:18.187+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T05:13:18.187+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T05:13:18.187+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T05:13:18.191+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="C:\\Users\\Madhav\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Madhav\\.ollama\\models\\blobs\\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 --threads 6 --no-mmap --parallel 1 --port 51810"
time=2025-04-04T05:13:18.255+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-04T05:13:18.255+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-04T05:13:18.256+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-04T05:13:18.287+05:30 level=INFO source=runner.go:821 msg="starting ollama engine"
time=2025-04-04T05:13:18.289+05:30 level=INFO source=runner.go:884 msg="Server listening on 127.0.0.1:51810"
time=2025-04-04T05:13:18.393+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
time=2025-04-04T05:13:18.394+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
time=2025-04-04T05:13:18.394+05:30 level=INFO source=ggml.go:66 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from C:\Users\Madhav\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
time=2025-04-04T05:13:18.411+05:30 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-04-04T05:13:18.415+05:30 level=INFO source=ggml.go:288 msg="model weights" buffer=CPU size="3.6 GiB"
time=2025-04-04T05:13:18.507+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-04T05:13:21.739+05:30 level=INFO source=ggml.go:380 msg="compute graph" backend=CPU buffer_type=CPU
time=2025-04-04T05:13:21.746+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T05:13:21.760+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T05:13:21.760+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T05:13:21.761+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T05:13:21.761+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T05:13:21.761+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T05:13:21.768+05:30 level=INFO source=server.go:619 msg="llama runner started in 3.51 seconds"
[GIN] 2025/04/04 - 05:13:34 | 200 |   16.5941843s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-04T05:18:39.505+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0141312 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T05:18:39.755+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2642444 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T05:18:40.005+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.5143212 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
[GIN] 2025/04/04 - 06:36:50 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 10:21:04 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 10:21:04 | 200 |     23.0053ms |       127.0.0.1 | GET      "/api/tags"
time=2025-04-04T11:28:37.461+05:30 level=INFO source=server.go:105 msg="system memory" total="13.9 GiB" free="2.1 GiB" free_swap="7.4 GiB"
time=2025-04-04T11:28:37.462+05:30 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=0 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.6 GiB" memory.required.partial="0 B" memory.required.kv="214.0 MiB" memory.required.allocations="[0 B]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-04-04T11:28:37.571+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T11:28:37.583+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T11:28:37.583+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T11:28:37.583+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T11:28:37.583+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T11:28:37.583+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T11:28:37.592+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="C:\\Users\\Madhav\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Madhav\\.ollama\\models\\blobs\\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 --threads 6 --no-mmap --parallel 1 --port 57912"
time=2025-04-04T11:28:37.697+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-04T11:28:37.697+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-04T11:28:37.698+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-04T11:28:37.742+05:30 level=INFO source=runner.go:821 msg="starting ollama engine"
time=2025-04-04T11:28:37.744+05:30 level=INFO source=runner.go:884 msg="Server listening on 127.0.0.1:57912"
time=2025-04-04T11:28:37.861+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
time=2025-04-04T11:28:37.861+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
time=2025-04-04T11:28:37.861+05:30 level=INFO source=ggml.go:66 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
time=2025-04-04T11:28:37.950+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from C:\Users\Madhav\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
time=2025-04-04T11:28:38.216+05:30 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-04-04T11:28:38.220+05:30 level=INFO source=ggml.go:288 msg="model weights" buffer=CPU size="3.6 GiB"
time=2025-04-04T11:28:49.432+05:30 level=INFO source=ggml.go:380 msg="compute graph" backend=CPU buffer_type=CPU
time=2025-04-04T11:28:49.442+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T11:28:49.455+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T11:28:49.455+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T11:28:49.455+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T11:28:49.455+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T11:28:49.455+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T11:28:49.498+05:30 level=INFO source=server.go:619 msg="llama runner started in 11.80 seconds"
[GIN] 2025/04/04 - 11:29:07 | 200 |   30.0814771s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-04T11:34:12.322+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.0306886 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T11:34:12.572+05:30 level=WARN source=sched.go:648 msg="gpu VRAM usage didn't recover within timeout" seconds=5.2808162 model=C:\Users\Madhav\.ollama\models\blobs\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25
time=2025-04-04T11:58:19.039+05:30 level=INFO source=server.go:105 msg="system memory" total="13.9 GiB" free="4.2 GiB" free_swap="8.1 GiB"
time=2025-04-04T11:58:19.040+05:30 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=0 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.6 GiB" memory.required.partial="0 B" memory.required.kv="214.0 MiB" memory.required.allocations="[0 B]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-04-04T11:58:19.151+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T11:58:19.164+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T11:58:19.164+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T11:58:19.164+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T11:58:19.164+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T11:58:19.164+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T11:58:19.171+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="C:\\Users\\Madhav\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Madhav\\.ollama\\models\\blobs\\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 --threads 6 --no-mmap --parallel 1 --port 58610"
time=2025-04-04T11:58:19.281+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-04T11:58:19.282+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-04T11:58:19.284+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-04T11:58:19.331+05:30 level=INFO source=runner.go:821 msg="starting ollama engine"
time=2025-04-04T11:58:19.334+05:30 level=INFO source=runner.go:884 msg="Server listening on 127.0.0.1:58610"
time=2025-04-04T11:58:19.437+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
time=2025-04-04T11:58:19.437+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
time=2025-04-04T11:58:19.437+05:30 level=INFO source=ggml.go:66 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from C:\Users\Madhav\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
time=2025-04-04T11:58:19.457+05:30 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-04-04T11:58:19.461+05:30 level=INFO source=ggml.go:288 msg="model weights" buffer=CPU size="3.6 GiB"
time=2025-04-04T11:58:19.536+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-04T11:58:27.973+05:30 level=INFO source=ggml.go:380 msg="compute graph" backend=CPU buffer_type=CPU
time=2025-04-04T11:58:27.980+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T11:58:27.991+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T11:58:27.991+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T11:58:27.991+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T11:58:27.991+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T11:58:27.991+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T11:58:28.056+05:30 level=INFO source=server.go:619 msg="llama runner started in 8.77 seconds"
[GIN] 2025/04/04 - 11:58:40 | 200 |   22.0281272s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/04 - 12:03:18 | 200 |     116.166ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/04 - 12:03:45 | 200 |    11.701979s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/04 - 12:04:23 | 200 |     28.99583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/04/04 - 12:08:45 | 200 |      3.0002ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 12:08:45 | 200 |      2.5008ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/04/04 - 12:08:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 12:08:51 | 404 |      1.0003ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-04T12:08:53.704+05:30 level=INFO source=download.go:177 msg="downloading 8934d96d3f08 in 16 239 MB part(s)"
[GIN] 2025/04/04 - 12:09:05 | 200 |   14.0860167s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/04 - 12:09:37 | 200 |       502.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/04 - 12:09:37 | 404 |       500.7µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/04 - 12:09:38 | 200 |    853.4992ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/04/04 - 12:09:38 | 200 |     117.434ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-04T12:09:38.616+05:30 level=INFO source=server.go:105 msg="system memory" total="13.9 GiB" free="5.2 GiB" free_swap="7.7 GiB"
time=2025-04-04T12:09:38.617+05:30 level=INFO source=server.go:138 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=0 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="2.6 GiB" memory.required.partial="0 B" memory.required.kv="214.0 MiB" memory.required.allocations="[0 B]" memory.weights.total="2.3 GiB" memory.weights.repeating="1.8 GiB" memory.weights.nonrepeating="525.0 MiB" memory.graph.full="517.0 MiB" memory.graph.partial="1.0 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-04-04T12:09:38.719+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T12:09:38.731+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T12:09:38.731+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T12:09:38.731+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T12:09:38.731+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T12:09:38.731+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T12:09:38.735+05:30 level=INFO source=server.go:405 msg="starting llama server" cmd="C:\\Users\\Madhav\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --ollama-engine --model C:\\Users\\Madhav\\.ollama\\models\\blobs\\sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --ctx-size 2048 --batch-size 512 --threads 6 --no-mmap --parallel 1 --port 58816"
time=2025-04-04T12:09:38.740+05:30 level=INFO source=sched.go:451 msg="loaded runners" count=1
time=2025-04-04T12:09:38.740+05:30 level=INFO source=server.go:580 msg="waiting for llama runner to start responding"
time=2025-04-04T12:09:38.741+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server error"
time=2025-04-04T12:09:38.785+05:30 level=INFO source=runner.go:821 msg="starting ollama engine"
time=2025-04-04T12:09:38.787+05:30 level=INFO source=runner.go:884 msg="Server listening on 127.0.0.1:58816"
time=2025-04-04T12:09:38.888+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
time=2025-04-04T12:09:38.888+05:30 level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
time=2025-04-04T12:09:38.888+05:30 level=INFO source=ggml.go:66 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=883 num_key_values=36
load_backend: loaded CPU backend from C:\Users\Madhav\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-haswell.dll
time=2025-04-04T12:09:38.905+05:30 level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(clang)
time=2025-04-04T12:09:38.910+05:30 level=INFO source=ggml.go:288 msg="model weights" buffer=CPU size="3.6 GiB"
time=2025-04-04T12:09:38.992+05:30 level=INFO source=server.go:614 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-04T12:09:47.906+05:30 level=INFO source=ggml.go:380 msg="compute graph" backend=CPU buffer_type=CPU
time=2025-04-04T12:09:47.915+05:30 level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
time=2025-04-04T12:09:47.928+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.attention.layer_norm_rms_epsilon default=9.999999974752427e-07
time=2025-04-04T12:09:47.928+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.local.freq_base default=10000
time=2025-04-04T12:09:47.928+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.global.freq_base default=1e+06
time=2025-04-04T12:09:47.928+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
time=2025-04-04T12:09:47.928+05:30 level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
time=2025-04-04T12:09:48.012+05:30 level=INFO source=server.go:619 msg="llama runner started in 9.27 seconds"
[GIN] 2025/04/04 - 12:09:54 | 200 |   16.5270932s |       127.0.0.1 | POST     "/api/generate"
